{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis on IMDB Reviews using LSTM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd    # to load dataset\n",
    "import numpy as np     # for mathematic equation\n",
    "from nltk.corpus import stopwords   # to get collection of stopwords\n",
    "from sklearn.model_selection import train_test_split       # for splitting dataset\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer  # to encode text to int\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences   # to do padding or truncating\n",
    "from tensorflow.keras.models import Sequential     # the model\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense # layers of the architecture\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint   # save model\n",
    "from tensorflow.keras.models import load_model   # load saved model\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<i>Preview dataset</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                  review sentiment\n",
      "0      One of the other reviewers has mentioned that ...  positive\n",
      "1      A wonderful little production. <br /><br />The...  positive\n",
      "2      I thought this was a wonderful way to spend ti...  positive\n",
      "3      Basically there's a family where a little boy ...  negative\n",
      "4      Petter Mattei's \"Love in the Time of Money\" is...  positive\n",
      "...                                                  ...       ...\n",
      "49995  I thought this movie did a down right good job...  positive\n",
      "49996  Bad plot, bad dialogue, bad acting, idiotic di...  negative\n",
      "49997  I am a Catholic taught in parochial elementary...  negative\n",
      "49998  I'm going to have to disagree with the previou...  negative\n",
      "49999  No one expects the Star Trek movies to be high...  negative\n",
      "\n",
      "[50000 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('IMDB Dataset.csv')\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<b>Stop Word</b> is a commonly used words in a sentence, usually a search engine is programmed to ignore this words (i.e. \"the\", \"a\", \"an\", \"of\", etc.)\n",
    "\n",
    "<i>Declaring the english stop words</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_stops = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## data cleaning\n",
    "\n",
    "<hr>\n",
    "\n",
    "### Load and Clean Dataset\n",
    "\n",
    "In the original dataset, the reviews are still dirty. There are still html tags, numbers, uppercase, and punctuations. This will not be good for training, so in <b>load_dataset()</b> function, beside loading the dataset using <b>pandas</b>, I also pre-process the reviews by removing html tags, non alphabet (punctuations and numbers), stop words, and lower case all of the reviews.\n",
    "\n",
    "### Encode Sentiments\n",
    "In the same function, I also encode the sentiments into integers (0 and 1). Where 0 is for negative sentiments and 1 is for positive sentiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "import nltk\n",
    "def wordopt(text_list):\n",
    "    cleaned_texts = []\n",
    "    v=nltk.word_tokenize(text_list)\n",
    "    for text in v:\n",
    "        # print(text)\n",
    "        text = text.lower()\n",
    "        text = re.sub('\\[.*?\\]', '', text)\n",
    "        text = re.sub(\"\\\\W\", \" \", text)\n",
    "        text = re.sub('https?://\\S+|www\\.\\S+', '', text)\n",
    "        text = re.sub('<.*?>+', '', text)\n",
    "        text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n",
    "        text = re.sub('\\n', '', text)\n",
    "        text = re.sub('\\w*\\d\\w*', '', text)\n",
    "        cleaned_texts.append(text)\n",
    "    return cleaned_texts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data[\"review\"].apply(wordopt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"review\"] = data[\"review\"].apply(wordopt)\n",
    "# data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[one, of, the, other, reviewers, has, mentione...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[a, wonderful, little, production,  ,  , br,  ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[i, thought, this, was, a, wonderful, way, to,...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[basically, there,  s, a, family, where, a, li...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[petter, mattei,  s,   , love, in, the, time, ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49995</th>\n",
       "      <td>[i, thought, this, movie, did, a, down, right,...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49996</th>\n",
       "      <td>[bad, plot,  , bad, dialogue,  , bad, acting, ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49997</th>\n",
       "      <td>[i, am, a, catholic, taught, in, parochial, el...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49998</th>\n",
       "      <td>[i,  m, going, to, have, to, disagree, with, t...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49999</th>\n",
       "      <td>[no, one, expects, the, star, trek, movies, to...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>50000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  review sentiment\n",
       "0      [one, of, the, other, reviewers, has, mentione...  positive\n",
       "1      [a, wonderful, little, production,  ,  , br,  ...  positive\n",
       "2      [i, thought, this, was, a, wonderful, way, to,...  positive\n",
       "3      [basically, there,  s, a, family, where, a, li...  negative\n",
       "4      [petter, mattei,  s,   , love, in, the, time, ...  positive\n",
       "...                                                  ...       ...\n",
       "49995  [i, thought, this, movie, did, a, down, right,...  positive\n",
       "49996  [bad, plot,  , bad, dialogue,  , bad, acting, ...  negative\n",
       "49997  [i, am, a, catholic, taught, in, parochial, el...  negative\n",
       "49998  [i,  m, going, to, have, to, disagree, with, t...  negative\n",
       "49999  [no, one, expects, the, star, trek, movies, to...  negative\n",
       "\n",
       "[50000 rows x 2 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "corpus = []\n",
    "\n",
    "for index, row in data.iterrows():\n",
    "    text_list = row['review']\n",
    "    \n",
    "    # Join the list of words into a single string\n",
    "    text = ' '.join(text_list)\n",
    "\n",
    "    # Remove non-alphabetic characters\n",
    "    text = re.sub('[^a-zA-Z]', ' ', text)\n",
    "\n",
    "    # Split the words\n",
    "    text = text.split()\n",
    "\n",
    "    # Lemmatize and remove stopwords\n",
    "    text = [lemmatizer.lemmatize(word) for word in text if word not in set(stopwords.words('english'))]\n",
    "\n",
    "    # Join the lemmatized words into a single string\n",
    "    text = ' '.join(text)\n",
    "\n",
    "    # Append the processed text to the corpus\n",
    "    corpus.append(text)\n",
    "\n",
    "# Assign the processed corpus back to the \"Comments\" column\n",
    "data[\"review\"] = corpus\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>one reviewer mentioned watching oz episode hoo...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>wonderful little production br br filming tech...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>thought wonderful way spend time hot summer we...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>basically family little boy jake think zombie ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>petter mattei love time money visually stunnin...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49995</th>\n",
       "      <td>thought movie right good job n creative origin...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49996</th>\n",
       "      <td>bad plot bad dialogue bad acting idiotic direc...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49997</th>\n",
       "      <td>catholic taught parochial elementary school nu...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49998</th>\n",
       "      <td>going disagree previous comment side maltin on...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49999</th>\n",
       "      <td>one expects star trek movie high art fan expec...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>50000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  review sentiment\n",
       "0      one reviewer mentioned watching oz episode hoo...  positive\n",
       "1      wonderful little production br br filming tech...  positive\n",
       "2      thought wonderful way spend time hot summer we...  positive\n",
       "3      basically family little boy jake think zombie ...  negative\n",
       "4      petter mattei love time money visually stunnin...  positive\n",
       "...                                                  ...       ...\n",
       "49995  thought movie right good job n creative origin...  positive\n",
       "49996  bad plot bad dialogue bad acting idiotic direc...  negative\n",
       "49997  catholic taught parochial elementary school nu...  negative\n",
       "49998  going disagree previous comment side maltin on...  negative\n",
       "49999  one expects star trek movie high art fan expec...  negative\n",
       "\n",
       "[50000 rows x 2 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## save the clean dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data\n",
    "data.to_csv('review_clean.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## check sentiment count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sentiment\n",
       "positive    25000\n",
       "negative    25000\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.sentiment.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## modeling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report,confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting the Words to Vector using tfidf vectorizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting the Words to Vector using tfidf vectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tf=TfidfVectorizer(ngram_range=(1,3),max_features=2500)\n",
    "X=tf.fit_transform(corpus).toarray()\n",
    "y=data['sentiment']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "### Split Dataset\n",
    "In this work, I decided to split the data into 80% of Training and 20% of Testing set using <b>train_test_split</b> method from Scikit-Learn. By using this method, it automatically shuffles the dataset. We need to shuffle the data because in the original dataset, the reviews and sentiments are in order, where they list positive reviews first and then negative reviews. By shuffling the data, it will be distributed equally in the model, so it will be more accurate for predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train,x_test,y_train,y_test=train_test_split(X,y,test_size=0.20,random_state=1,stratify=y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model artitecture model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "### Build Architecture/Model\n",
    "<b>Embedding Layer</b>: in simple terms, it creates word vectors of each word in the <i>word_index</i> and group words that are related or have similar meaning by analyzing other words around them.\n",
    "\n",
    "<b>LSTM Layer</b>: to make a decision to keep or throw away data by considering the current input, previous output, and previous memory. There are some important components in LSTM.\n",
    "<ul>\n",
    "    <li><b>Forget Gate</b>, decides information is to be kept or thrown away</li>\n",
    "    <li><b>Input Gate</b>, updates cell state by passing previous output and current input into sigmoid activation function</li>\n",
    "    <li><b>Cell State</b>, calculate new cell state, it is multiplied by forget vector (drop value if multiplied by a near 0), add it with the output from input gate to update the cell state value.</li>\n",
    "    <li><b>Ouput Gate</b>, decides the next hidden state and used for predictions</li>\n",
    "</ul>\n",
    "\n",
    "<b>Dense Layer</b>: compute the input with the weight matrix and bias (optional), and using an activation function. I use <b>Sigmoid</b> activation function for this work because the output is only 0 or 1.\n",
    "\n",
    "The optimizer is <b>Adam</b> and the loss function is <b>categorical Crossentropy</b> because again the output is only 0 and 1, which is a binary number."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "### Tokenize and Pad/Truncate Reviews\n",
    "A Neural Network only accepts numeric data, so we need to encode the reviews. I use <b>tensorflow.keras.preprocessing.text.Tokenizer</b> to encode the reviews into integers, where each unique word is automatically indexed (using <b>fit_on_texts</b> method) based on <b>x_train</b>. <br>\n",
    "<b>x_train</b> and <b>x_test</b> is converted into integers using <b>texts_to_sequences</b> method.\n",
    "\n",
    "Each reviews has a different length, so we need to add padding (by adding 0) or truncating the words to the same length (in this case, it is the mean of all reviews length) using <b>tensorflow.keras.preprocessing.sequence.pad_sequences</b>.\n",
    "\n",
    "\n",
    "<b>post</b>, pad or truncate the words in the back of a sentence<br>\n",
    "<b>pre</b>, pad or truncate the words in front of a sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m119s\u001b[0m 117ms/step - accuracy: 0.6465 - loss: 0.5752 - val_accuracy: 0.8719 - val_loss: 0.3083\n",
      "Epoch 2/10\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m110s\u001b[0m 110ms/step - accuracy: 0.8992 - loss: 0.2507 - val_accuracy: 0.8745 - val_loss: 0.2970\n",
      "Epoch 3/10\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m111s\u001b[0m 111ms/step - accuracy: 0.9190 - loss: 0.2056 - val_accuracy: 0.8605 - val_loss: 0.3315\n",
      "Epoch 4/10\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m119s\u001b[0m 119ms/step - accuracy: 0.9310 - loss: 0.1818 - val_accuracy: 0.8646 - val_loss: 0.3261\n",
      "Epoch 5/10\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m113s\u001b[0m 113ms/step - accuracy: 0.9410 - loss: 0.1582 - val_accuracy: 0.8675 - val_loss: 0.3501\n",
      "Epoch 6/10\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m120s\u001b[0m 120ms/step - accuracy: 0.9498 - loss: 0.1377 - val_accuracy: 0.8558 - val_loss: 0.3989\n",
      "Epoch 7/10\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m120s\u001b[0m 120ms/step - accuracy: 0.9554 - loss: 0.1209 - val_accuracy: 0.8561 - val_loss: 0.3959\n",
      "Epoch 8/10\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m120s\u001b[0m 120ms/step - accuracy: 0.9628 - loss: 0.1067 - val_accuracy: 0.8559 - val_loss: 0.4807\n",
      "Epoch 9/10\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m122s\u001b[0m 122ms/step - accuracy: 0.9712 - loss: 0.0838 - val_accuracy: 0.8546 - val_loss: 0.4446\n",
      "Epoch 10/10\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m124s\u001b[0m 125ms/step - accuracy: 0.9754 - loss: 0.0730 - val_accuracy: 0.8551 - val_loss: 0.5916\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 41ms/step\n",
      "Accuracy: 0.8585\n",
      "Precision: 0.8584993069306929\n",
      "Recall: 0.8585\n",
      "F1 Score: 0.8584986146052628\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.86      0.86      0.86      4961\n",
      "    positive       0.86      0.86      0.86      5039\n",
      "\n",
      "    accuracy                           0.86     10000\n",
      "   macro avg       0.86      0.86      0.86     10000\n",
      "weighted avg       0.86      0.86      0.86     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report, accuracy_score, precision_recall_fscore_support\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Embedding, Flatten, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer  # to encode text to int\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences   # to do padding or truncating\n",
    "\n",
    "# Load the Amazon review dataset\n",
    "dataset_path = 'review_clean.csv'\n",
    "df = pd.read_csv(dataset_path)\n",
    "\n",
    "# Preprocess the data\n",
    "max_words = 10000\n",
    "max_len = 256\n",
    "# Tokenize and pad sequences (assuming you are using a simple Tokenizer)\n",
    "tokenizer = Tokenizer(num_words=max_words, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(df['review'])\n",
    "sequences = tokenizer.texts_to_sequences(df['review'])\n",
    "padded_sequences = pad_sequences(sequences, maxlen=max_len, padding='post', truncating='post')\n",
    "\n",
    "# Convert string labels to numerical values\n",
    "label_encoder = LabelEncoder()\n",
    "df['sentiment'] = label_encoder.fit_transform(df['sentiment'])\n",
    "num_classes = len(label_encoder.classes_)\n",
    "\n",
    "labels = np.array(df['sentiment'])\n",
    "\n",
    "# Convert labels to categorical for multiclass classification\n",
    "y_train = to_categorical(labels, num_classes=num_classes)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "x_train, x_test, y_train, y_test = train_test_split(padded_sequences, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "# Build the model\n",
    "\n",
    "embedding_dim = 128\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=max_words, output_dim=embedding_dim))  # Remove input_length here\n",
    "model.add(LSTM(64, return_sequences=True))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(learning_rate=1e-4), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(x_train, y_train, epochs=10, batch_size=32, validation_split=0.2)\n",
    "\n",
    "\n",
    "# Evaluate on the test set\n",
    "lstm_y_pred = np.argmax(model.predict(x_test), axis=-1)\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy = accuracy_score(np.argmax(y_test, axis=-1), lstm_y_pred)\n",
    "precision, recall, f1, _ = precision_recall_fscore_support(np.argmax(y_test, axis=-1), lstm_y_pred, average='weighted')\n",
    "\n",
    "# Print metrics\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1 Score: {f1}\")\n",
    "\n",
    "# Generate classification report\n",
    "class_names = [str(label) for label in label_encoder.classes_]\n",
    "# Generate classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(np.argmax(y_test, axis=-1), lstm_y_pred, target_names=class_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8585\n"
     ]
    }
   ],
   "source": [
    "print(f\"Accuracy: {accuracy}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "# Save the model\n",
    "model.save('sentiment_analysis_model.h5')\n",
    "\n",
    "# Save the tokenizer\n",
    "with open('tokenizer.pickle', 'wb') as handle:\n",
    "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "# Save the label encoder\n",
    "with open('label_encoder.pickle', 'wb') as handle:\n",
    "    pickle.dump(label_encoder, handle, protocol=pickle.HIGHEST_PROTOCOL)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## user prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 169ms/step\n",
      "Predicted sentiment: positive\n"
     ]
    }
   ],
   "source": [
    "def predict_sentiment(review):\n",
    "    # Load tokenizer and label encoder\n",
    "    with open('tokenizer.pickle', 'rb') as handle:\n",
    "        tokenizer = pickle.load(handle)\n",
    "\n",
    "    with open('label_encoder.pickle', 'rb') as handle:\n",
    "        label_encoder = pickle.load(handle)\n",
    "\n",
    "    # Preprocess the user input\n",
    "    sequence = tokenizer.texts_to_sequences([review])\n",
    "    padded_sequence = pad_sequences(sequence, maxlen=256, padding='post', truncating='post')\n",
    "\n",
    "    # Load the model\n",
    "    model = load_model('sentiment_analysis_model.h5')\n",
    "\n",
    "    # Make prediction\n",
    "    prediction = model.predict(padded_sequence)\n",
    "    predicted_class = label_encoder.inverse_transform([np.argmax(prediction)])\n",
    "\n",
    "    return predicted_class[0]\n",
    "\n",
    "# Example usage:\n",
    "user_review = \"Hatts of to JAMES CAMERON for thinking and creating a vision like this. It takes a lotseof hardwork & research to build a whole new world. And there is a reason why this movie is still the no 1 movie in the world. I've never seen this kind of visuals in any other film. And this movie was made in 2009 that was an amazing achievement by the VFX creators & the director itself. You can easily get connected with the movie plot and the way the director has shown the Pandora world was just unbelievable. Cast of this film has done a fabulous job while performing so well and get into the character that not a single one will disappoint. Emotional scenes are so powerful that you feel the characters and their pain for what their suffering. Visual Effects makes this movie so powerful that every creature and big scenery scenes looks real. That's why it has re-released once again to feel the same experience. Don't miss this one on the big screen if you haven't seen it. It's a total new world experience. Can't wait for the PART 2\"\n",
    "predicted_sentiment = predict_sentiment(user_review)\n",
    "print(\"Predicted sentiment:\", predicted_sentiment)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 403ms/step\n",
      "Prediction: 1\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import string\n",
    "import pickle\n",
    "import nltk\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Load the LSTM model\n",
    "from tensorflow.keras.models import load_model\n",
    "model = load_model('sentiment_analysis_model.h5')\n",
    "\n",
    "# Load the tokenizer and label encoder\n",
    "with open('tokenizer.pickle', 'rb') as handle:\n",
    "    tokenizer = pickle.load(handle)\n",
    "\n",
    "# Define the maximum sequence length\n",
    "max_len = 256\n",
    "\n",
    "def preprocess_input(text):\n",
    "    # Clean the text using regular expressions\n",
    "    text = re.sub(r'https?://\\S+|www\\.\\S+', '', text)  # Remove URLs\n",
    "    text = re.sub(r'<.*?>+', '', text)  # Remove HTML tags\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)  # Remove punctuation\n",
    "    text = re.sub(r'\\d', '', text)  # Remove digits\n",
    "    text = text.lower()  # Convert to lowercase\n",
    "    return text\n",
    "\n",
    "def preprocess_and_predict(user_input):\n",
    "    # Preprocess user input\n",
    "    preprocessed_input = preprocess_input(user_input)\n",
    "\n",
    "    # Tokenize and pad the preprocessed input sequence\n",
    "    sequence = tokenizer.texts_to_sequences([preprocessed_input])\n",
    "    padded_sequence = pad_sequences(sequence, maxlen=256, padding='post', truncating='post')\n",
    "\n",
    "    # Predict using the LSTM model\n",
    "    prediction = model.predict(padded_sequence)\n",
    "\n",
    "    return np.argmax(prediction)  # Return the index of the predicted class\n",
    "\n",
    "# Get user input\n",
    "user_input = input('Enter Your Message: ')\n",
    "\n",
    "# Predict and print the result\n",
    "result = preprocess_and_predict(user_input)\n",
    "print(\"Prediction:\", result)\n",
    "\n",
    "# positive - 1\n",
    "# negative - 0\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
